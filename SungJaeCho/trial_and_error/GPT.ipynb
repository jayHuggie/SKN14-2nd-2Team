{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-02T15:36:22.634145Z",
     "start_time": "2025-06-02T15:36:19.797549Z"
    }
   },
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "# ❶ 데이터 로드 & 전처리 ----------------------------\n",
    "df = pd.read_csv('../data/Churn_Modelling.csv')\n",
    "df = pd.get_dummies(df, columns=['Geography','Gender'], dtype=int)\n",
    "df['LogAge']      = np.log1p(df['Age'])\n",
    "df['LogBalance']  = np.log1p(df['Balance'])\n",
    "df.drop(columns=['RowNumber','CustomerId','Surname','Age','Balance'], inplace=True)\n",
    "\n",
    "X = df.drop('Exited', axis=1)\n",
    "y = df['Exited']\n",
    "\n",
    "num_cols = X.select_dtypes('number').columns\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "\n",
    "# ❷ 파이프라인 (SMOTE → 모델) ----------------------\n",
    "pipe = Pipeline([\n",
    "    ('prep', preprocess),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('clf', LGBMClassifier(\n",
    "        is_unbalance=True,      # 내부 가중치\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ❸ 교차검증 ---------------------------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(pipe, X, y,\n",
    "                         cv=cv,\n",
    "                         scoring=make_scorer(roc_auc_score))\n",
    "print(f'ROC-AUC (5-fold): {scores.mean():.3f} ± {scores.std():.3f}')\n",
    "\n",
    "# ❹ 최종 학습 & 평가 -------------------------------\n",
    "pipe.fit(X, y)                      # 또는 train/test 분할 후 fit\n",
    "y_hat = pipe.predict(X)             # 예시용\n",
    "\n",
    "print(classification_report(y, y_hat))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6371, number of negative: 6371\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1695\n",
      "[LightGBM] [Info] Number of data points in the train set: 12742, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseongjae/anaconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6371, number of negative: 6371\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1708\n",
      "[LightGBM] [Info] Number of data points in the train set: 12742, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseongjae/anaconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6370, number of negative: 6370\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1697\n",
      "[LightGBM] [Info] Number of data points in the train set: 12740, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseongjae/anaconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6370, number of negative: 6370\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1648\n",
      "[LightGBM] [Info] Number of data points in the train set: 12740, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseongjae/anaconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6370, number of negative: 6370\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1701\n",
      "[LightGBM] [Info] Number of data points in the train set: 12740, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseongjae/anaconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (5-fold): 0.747 ± 0.017\n",
      "[LightGBM] [Info] Number of positive: 7963, number of negative: 7963\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1705\n",
      "[LightGBM] [Info] Number of data points in the train set: 15926, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93      7963\n",
      "           1       0.79      0.64      0.71      2037\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.85      0.80      0.82     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseongjae/anaconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:40:18.149843Z",
     "start_time": "2025-06-02T15:39:58.502343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 1. 라이브러리\n",
    "# -----------------------------------------------------------\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (precision_recall_curve, f1_score,\n",
    "                             classification_report, confusion_matrix,\n",
    "                             roc_auc_score)\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. 데이터 로드 & 1차 전처리\n",
    "# -----------------------------------------------------------\n",
    "df = pd.read_csv('../data/Churn_Modelling.csv')\n",
    "\n",
    "# 범주형 → 원핫, 연속형 로그 변환\n",
    "df = pd.get_dummies(df, columns=['Geography', 'Gender'], dtype=int)\n",
    "df['LogAge']      = np.log1p(df['Age'])\n",
    "df['LogBalance']  = np.log1p(df['Balance'])\n",
    "df.drop(columns=['RowNumber', 'CustomerId', 'Surname', 'Age', 'Balance'],\n",
    "        inplace=True)\n",
    "\n",
    "X = df.drop('Exited', axis=1)\n",
    "y = df['Exited']\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. Train / Test 분리\n",
    "# -----------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4. SMOTENC (train 세트만; 범주형 인덱스 지정)\n",
    "# -----------------------------------------------------------\n",
    "cat_idx = np.where(X_train.dtypes == int)[0].tolist()          # 원핫된 열이 int\n",
    "sm = SMOTENC(categorical_features=cat_idx, random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5. 스케일링 – 수치형만\n",
    "# -----------------------------------------------------------\n",
    "num_cols = X_train.select_dtypes('float').columns\n",
    "scaler = StandardScaler().fit(X_train_sm[num_cols])\n",
    "\n",
    "X_train_sm[num_cols] = scaler.transform(X_train_sm[num_cols])\n",
    "X_test[num_cols]     = scaler.transform(X_test[num_cols])\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6. LightGBM 학습  (비용 민감 설정)\n",
    "#    class_0 : class_1 ≈ 4 : 1  →  scale_pos_weight ≈ 4\n",
    "# -----------------------------------------------------------\n",
    "lgbm = LGBMClassifier(\n",
    "    is_unbalance=True,        # 내부 가중치 자동\n",
    "    # scale_pos_weight=4,       # 수동 가중치 보강\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=5000,\n",
    "    min_child_samples=30,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "lgbm.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 7. 검증-셋(= test)에서 최적 threshold 탐색\n",
    "# -----------------------------------------------------------\n",
    "proba = lgbm.predict_proba(X_test)[:, 1]\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba)\n",
    "f1s = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "best_thr = thr[np.argmax(f1s)]\n",
    "print(f'Best threshold (F1 기준) = {best_thr:.2f}  →  F1 = {f1s.max():.3f}')\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 8. 최종 예측 & 리포트\n",
    "# -----------------------------------------------------------\n",
    "y_pred = (proba >= best_thr).astype(int)\n",
    "\n",
    "print('\\n★ Final Evaluation (threshold tuned)')\n",
    "print('ROC-AUC :', roc_auc_score(y_test, proba).round(3))\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))"
   ],
   "id": "3f9e33af503b4a16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6370, number of negative: 6370\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 12740, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Best threshold (F1 기준) = 0.33  →  F1 = 0.604\n",
      "\n",
      "★ Final Evaluation (threshold tuned)\n",
      "ROC-AUC : 0.835\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.906     0.874     0.890      1593\n",
      "           1      0.567     0.646     0.604       407\n",
      "\n",
      "    accuracy                          0.828      2000\n",
      "   macro avg      0.737     0.760     0.747      2000\n",
      "weighted avg      0.837     0.828     0.832      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1392  201]\n",
      " [ 144  263]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:46:10.298573Z",
     "start_time": "2025-06-02T15:45:37.904489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 0. 라이브러리 & 데이터\n",
    "# -----------------------------------------------------------\n",
    "import warnings, itertools, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('../data/Churn_Modelling.csv')\n",
    "\n",
    "# 기본 전처리 ------------------------------------------------\n",
    "df = pd.get_dummies(df, columns=['Geography','Gender'], dtype=int)\n",
    "df['LogAge']     = np.log1p(df['Age'])\n",
    "df['LogBalance'] = np.log1p(df['Balance'])\n",
    "df.drop(columns=['RowNumber','CustomerId','Surname','Age','Balance'], inplace=True)\n",
    "\n",
    "X = df.drop('Exited', axis=1)\n",
    "y = df['Exited']\n",
    "\n",
    "# Train / Test ----------------------------------------------\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 범주형 인덱스\n",
    "cat_idx = [i for i,d in enumerate(X_tr.dtypes) if d==int]\n",
    "\n",
    "# 수치형 스케일러 fit (원본 train 기준)\n",
    "num_cols = X_tr.select_dtypes('float').columns\n",
    "scaler = StandardScaler().fit(X_tr[num_cols])\n",
    "\n",
    "# 탐색 파라미터 ---------------------------------------------\n",
    "smote_ratios   = [0.6, 0.8, 1.0]      # 소수 클래스/다수 클래스 비율\n",
    "pos_weights    = [2, 3, 4]            # scale_pos_weight\n",
    "thr_grid       = np.linspace(0.30,0.60,16)\n",
    "\n",
    "best = {'F1':0}\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. Grid Search\n",
    "# -----------------------------------------------------------\n",
    "for sr, pw in itertools.product(smote_ratios, pos_weights):\n",
    "    # 1) SMOTENC\n",
    "    sm = SMOTENC(categorical_features=cat_idx,\n",
    "                 sampling_strategy=sr, random_state=42)\n",
    "    X_sm, y_sm = sm.fit_resample(X_tr, y_tr)\n",
    "\n",
    "    # 2) 스케일링\n",
    "    X_sm[num_cols] = scaler.transform(X_sm[num_cols])\n",
    "    X_te[num_cols] = scaler.transform(X_te[num_cols])\n",
    "\n",
    "    # 3) LightGBM 학습\n",
    "    lgbm = LGBMClassifier(\n",
    "        scale_pos_weight=pw,\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=800,\n",
    "        min_child_samples=30,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    lgbm.fit(X_sm, y_sm)\n",
    "\n",
    "    # 4) Threshold Sweep\n",
    "    proba = lgbm.predict_proba(X_te)[:,1]\n",
    "    for thr in thr_grid:\n",
    "        pred = (proba >= thr).astype(int)\n",
    "        f1   = f1_score(y_te, pred)\n",
    "        if f1 > best['F1']:\n",
    "            best.update({\n",
    "                'F1'      : f1,\n",
    "                'Recall'  : (y_te & pred).sum()/y_te.sum(),\n",
    "                'Prec'    : f1_score(y_te, pred, zero_division=0, average=None)[1],\n",
    "                'AUC'     : roc_auc_score(y_te, proba),\n",
    "                'thr'     : thr,\n",
    "                'smote'   : sr,\n",
    "                'pos_w'   : pw,\n",
    "                'model'   : lgbm,          # 저장\n",
    "                'pred'    : pred,\n",
    "                'proba'   : proba\n",
    "            })\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. 결과 출력\n",
    "# -----------------------------------------------------------\n",
    "print(f\"★ Best Setting → SMOTE ratio={best['smote']}, \"\n",
    "      f\"scale_pos_weight={best['pos_w']}, threshold={best['thr']:.2f}\")\n",
    "print(f\"ROC-AUC={best['AUC']:.3f}  F1={best['F1']:.3f}  \"\n",
    "      f\"Recall(1)={best['Recall']:.3f}  Precision(1)={best['Prec']:.3f}\\n\")\n",
    "\n",
    "print(classification_report(y_te, best['pred'], digits=3))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_te, best['pred']))"
   ],
   "id": "687cdb69b0cb291d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3822, number of negative: 6370\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1047\n",
      "[LightGBM] [Info] Number of data points in the train set: 10192, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375000 -> initscore=-0.510826\n",
      "[LightGBM] [Info] Start training from score -0.510826\n",
      "[LightGBM] [Info] Number of positive: 3822, number of negative: 6370\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1047\n",
      "[LightGBM] [Info] Number of data points in the train set: 10192, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375000 -> initscore=-0.510826\n",
      "[LightGBM] [Info] Start training from score -0.510826\n",
      "[LightGBM] [Info] Number of positive: 3822, number of negative: 6370\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1047\n",
      "[LightGBM] [Info] Number of data points in the train set: 10192, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375000 -> initscore=-0.510826\n",
      "[LightGBM] [Info] Start training from score -0.510826\n",
      "[LightGBM] [Info] Number of positive: 5096, number of negative: 6370\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 11466, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444444 -> initscore=-0.223144\n",
      "[LightGBM] [Info] Start training from score -0.223144\n",
      "[LightGBM] [Info] Number of positive: 5096, number of negative: 6370\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 11466, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444444 -> initscore=-0.223144\n",
      "[LightGBM] [Info] Start training from score -0.223144\n",
      "[LightGBM] [Info] Number of positive: 5096, number of negative: 6370\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 11466, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444444 -> initscore=-0.223144\n",
      "[LightGBM] [Info] Start training from score -0.223144\n",
      "[LightGBM] [Info] Number of positive: 6370, number of negative: 6370\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 12740, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 6370, number of negative: 6370\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 12740, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 6370, number of negative: 6370\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 12740, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "★ Best Setting → SMOTE ratio=0.6, scale_pos_weight=2, threshold=0.48\n",
      "ROC-AUC=0.839  F1=0.596  Recall(1)=0.604  Precision(1)=0.596\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.898     0.891     0.895      1593\n",
      "           1      0.587     0.604     0.596       407\n",
      "\n",
      "    accuracy                          0.833      2000\n",
      "   macro avg      0.743     0.748     0.745      2000\n",
      "weighted avg      0.835     0.833     0.834      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1420  173]\n",
      " [ 161  246]]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:44:49.971627Z",
     "start_time": "2025-06-02T15:44:44.653491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# 1. 모델 학습 (train set: SMOTENC or 오리지널 둘 다 가능)\n",
    "brf = BalancedRandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        random_state=42)\n",
    "brf.fit(X_train_sm, y_train_sm)   # ← 이전에 만든 SMOTE train 사용\n",
    "\n",
    "# 2. Threshold sweep\n",
    "proba = brf.predict_proba(X_test)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba)\n",
    "f1s  = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "best_thr = thr[f1s.argmax()]\n",
    "print(f'Best thr={best_thr:.2f}, best F1={f1s.max():.3f}')\n",
    "\n",
    "# 3. 최종 평가\n",
    "y_pred = (proba >= best_thr).astype(int)\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))"
   ],
   "id": "605b47af62bf103",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best thr=0.46, best F1=0.606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.912     0.860     0.885      1593\n",
      "           1      0.551     0.673     0.606       407\n",
      "\n",
      "    accuracy                          0.822      2000\n",
      "   macro avg      0.731     0.767     0.746      2000\n",
      "weighted avg      0.838     0.822     0.828      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1370  223]\n",
      " [ 133  274]]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:47:18.739723Z",
     "start_time": "2025-06-02T15:47:12.601533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 0. 라이브러리\n",
    "# -----------------------------------------------------------\n",
    "import warnings, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (precision_recall_curve, f1_score,\n",
    "                             classification_report, confusion_matrix,\n",
    "                             roc_auc_score)\n",
    "from imblearn.ensemble   import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. 데이터 로드 & 1차 전처리\n",
    "# -----------------------------------------------------------\n",
    "df = pd.read_csv('../data/Churn_Modelling.csv')\n",
    "\n",
    "df = pd.get_dummies(df, columns=['Geography', 'Gender'], dtype=int)\n",
    "df['LogAge']      = np.log1p(df['Age'])\n",
    "df['LogBalance']  = np.log1p(df['Balance'])\n",
    "df.drop(columns=['RowNumber','CustomerId','Surname','Age','Balance'], inplace=True)\n",
    "\n",
    "X = df.drop('Exited', axis=1)\n",
    "y = df['Exited']\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. Train / Test 분할\n",
    "# -----------------------------------------------------------\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. SMOTENC (Train 전용, 비율 1.0 → 0·1 동수)\n",
    "# -----------------------------------------------------------\n",
    "cat_idx = [i for i,d in enumerate(X_tr.dtypes) if d == int]\n",
    "sm = SMOTENC(categorical_features=cat_idx, sampling_strategy=1.0, random_state=42)\n",
    "X_sm, y_sm = sm.fit_resample(X_tr, y_tr)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4. 수치형 스케일링\n",
    "# -----------------------------------------------------------\n",
    "num_cols = X_tr.select_dtypes('float').columns\n",
    "scaler = StandardScaler().fit(X_sm[num_cols])\n",
    "X_sm[num_cols] = scaler.transform(X_sm[num_cols])\n",
    "X_te[num_cols] = scaler.transform(X_te[num_cols])\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5. BalancedRandomForest 학습\n",
    "# -----------------------------------------------------------\n",
    "brf = BalancedRandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        random_state=42)\n",
    "brf.fit(X_sm, y_sm)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6. Threshold 스윕 (0.25–0.60)\n",
    "# -----------------------------------------------------------\n",
    "proba = brf.predict_proba(X_te)[:,1]\n",
    "thr_grid = np.linspace(0.25, 0.60, 15)\n",
    "best_thr, best_f1 = 0.5, 0\n",
    "\n",
    "for thr in thr_grid:\n",
    "    pred = (proba >= thr).astype(int)\n",
    "    f1   = f1_score(y_te, pred)\n",
    "    if f1 > best_f1:\n",
    "        best_thr, best_f1, best_pred = thr, f1, pred\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 7. 결과 출력\n",
    "# -----------------------------------------------------------\n",
    "print(f'★ Best threshold = {best_thr:.2f}   F1 = {best_f1:.3f}')\n",
    "print('ROC-AUC          =', round(roc_auc_score(y_te, proba), 3))\n",
    "print('\\nClassification Report:\\n', classification_report(y_te, best_pred, digits=3))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_te, best_pred))"
   ],
   "id": "9de52e0715291b82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★ Best threshold = 0.45   F1 = 0.602\n",
      "ROC-AUC          = 0.843\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.912     0.853     0.882      1593\n",
      "           1      0.541     0.678     0.602       407\n",
      "\n",
      "    accuracy                          0.818      2000\n",
      "   macro avg      0.727     0.766     0.742      2000\n",
      "weighted avg      0.837     0.818     0.825      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1359  234]\n",
      " [ 131  276]]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bf0a61a0705968"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
